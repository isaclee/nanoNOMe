Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 8
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	download_data
	1	pares_hg38_cgi
	1	parse_downloaded_data
	3

[Mon Jan 28 13:07:24 2019]
rule download_data:
    output: download/hg38_cgi.txt.gz.download
    jobid: 6
    wildcards: sample=hg38_cgi.txt.gz

wget -q http://hgdownload.soe.ucsc.edu/goldenPath/hg38/database/cpgIslandExtUnmasked.txt.gz -O download/hg38_cgi.txt.gz.download
[Mon Jan 28 13:07:30 2019]
Finished job 6.
1 of 3 steps (33%) done

[Mon Jan 28 13:07:30 2019]
rule pares_hg38_cgi:
    input: download/hg38_cgi.txt.gz.download
    output: data/hg38/hg38_cgi.bed
    jobid: 2

gunzip -c download/hg38_cgi.txt.gz.download | awk ';OFS="	"{ print $2,$3,$4 } | sort -k1,1 -k2,2n > data/hg38/hg38_cgi.bed
[Mon Jan 28 13:07:30 2019]
Error in rule pares_hg38_cgi:
    jobid: 2
    output: data/hg38/hg38_cgi.bed

RuleException:
CalledProcessError in line 82 of /home/isac/Code/nanoNOMe/snakemake/downloaded_data_parse.smk:
Command ' set -euo pipefail;  gunzip -c download/hg38_cgi.txt.gz.download | awk ';OFS="	"{ print $2,$3,$4 } | sort -k1,1 -k2,2n > data/hg38/hg38_cgi.bed ' returned non-zero exit status 1.
  File "/home/isac/Code/nanoNOMe/snakemake/downloaded_data_parse.smk", line 82, in __rule_pares_hg38_cgi
  File "/home/isac/.conda/envs/isacenv/lib/python3.6/concurrent/futures/thread.py", line 56, in run
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /home/isac/Code/nanoNOMe/.snakemake/log/2019-01-28T130720.621015.snakemake.log
