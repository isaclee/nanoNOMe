Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 8
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	pares_hg38_cgi
	1	parse_downloaded_data
	2

[Mon Jan 28 13:08:04 2019]
rule pares_hg38_cgi:
    input: download/hg38_cgi.txt.gz.download
    output: data/hg38/hg38_cgi.bed
    jobid: 1

gunzip -c download/hg38_cgi.txt.gz.download | awk 'OFS="	"{ print $2,$3,$4 } | sort -k1,1 -k2,2n > data/hg38/hg38_cgi.bed
[Mon Jan 28 13:08:04 2019]
Error in rule pares_hg38_cgi:
    jobid: 1
    output: data/hg38/hg38_cgi.bed

RuleException:
CalledProcessError in line 82 of /home/isac/Code/nanoNOMe/snakemake/downloaded_data_parse.smk:
Command ' set -euo pipefail;  gunzip -c download/hg38_cgi.txt.gz.download | awk 'OFS="	"{ print $2,$3,$4 } | sort -k1,1 -k2,2n > data/hg38/hg38_cgi.bed ' returned non-zero exit status 1.
  File "/home/isac/Code/nanoNOMe/snakemake/downloaded_data_parse.smk", line 82, in __rule_pares_hg38_cgi
  File "/home/isac/.conda/envs/isacenv/lib/python3.6/concurrent/futures/thread.py", line 56, in run
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /home/isac/Code/nanoNOMe/.snakemake/log/2019-01-28T130801.283836.snakemake.log
