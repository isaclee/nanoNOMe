Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 8
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	pares_hg38_cgi
	1	parse_downloaded_data
	2

[Mon Jan 28 13:08:42 2019]
rule pares_hg38_cgi:
    input: download/hg38_cgi.txt.gz.download
    output: data/hg38/hg38_cgi.bed
    jobid: 3

gunzip -c download/hg38_cgi.txt.gz.download | awk 'OFS="	"{ print $2,$3,$4 }' | sort -k1,1 -k2,2n > data/hg38/hg38_cgi.bed
[Mon Jan 28 13:08:42 2019]
Finished job 3.
1 of 2 steps (50%) done

[Mon Jan 28 13:08:42 2019]
localrule parse_downloaded_data:
    input: data/gm12878/GM12878_CTCF.center.2000bp.bed, data/hg38/hg38_genes.TSS.400bp.bed, data/hg38/hg38_cgi.bed
    jobid: 0

[Mon Jan 28 13:08:42 2019]
Finished job 0.
2 of 2 steps (100%) done
Complete log: /home/isac/Code/nanoNOMe/.snakemake/log/2019-01-28T130839.401215.snakemake.log
